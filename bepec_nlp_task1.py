# -*- coding: utf-8 -*-
"""Bepec_NLP_Task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oVStS_jhS6v0rvsD93LG-r3HOrvymfT6
"""

import pandas as pd
import numpy as np
import nltk
# Cleaning the texts
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from joblib import dump, load
import pickle





#data = pd.read_excel("1000 leads.xlsx")


'''#pd.read_excel(io.StringIO(uploaded['1000 leads.xlsx'].decode('utf-8')))
from google.colab import files 
import io
uploaded = files.upload()
data = io.BytesIO(uploaded['1000 leads.xlsx'])'''

df=pd.read_excel("1000 leads.xlsx")

df.head()

df.columns

df.drop(['Lead Name', 'Location','Unnamed: 4'],axis=1, inplace=True)

df.head()

df.info

df['Status ']= df['Status '].replace(['Conveted'],'Converted ')

df['Status '].unique()

data_new=df.dropna()

data_new.isnull().sum().sort_values(ascending = False)

data_new.head()

"""[link text](https:// [link text](https://))# New Section"""

data_new.columns

data_new['Status ']

data_new['Status '].value_counts(normalize=True) * 100

data_new['Status1']=data_new['Status ']

data_new['Status_information']=data_new['Status information']

data_new.drop(['Status '],axis =1 , inplace=True)

data_new.drop('Status information',axis =1 , inplace=True)

data_new['Status1'] = data_new['Status1'].replace([''],'converted')

data_new['Status1'].unique()

import re
import nltk
nltk.download('stopwords')

data_new.reset_index(drop=True, inplace=True)
data_new.head(10)

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
corpus = []
for i in range(0, len(data_new)):
    review = re.sub('[^a-zA-Z]', ' ', data_new['Status_information'][i])
    review = review.lower()
    review = review.split()
    
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2500)
X = cv.fit_transform(corpus).toarray()
pickle.dump(cv,open('transform2.pkl','wb'))



# Apply first level cleaning
import re
import string

#This function converts to lower-case, removes square bracket, removes numbers and punctuation
def text_clean_1(text):
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

cleaned1 = lambda x: text_clean_1(x)

# Let's take a look at the updated text
data_new['Status1'] = pd.DataFrame(data_new.Status1.apply(cleaned1))
data_new.head(10)

data_new['Status1']= data_new['Status1'].replace(['converted '],'interested')
data_new['Status1']= data_new['Status1'].replace(['not converted'],'not interested')

data_new['Status1'].unique()

data_new['Status1'].value_counts(normalize=True) * 100



data_new.dropna()
Y=data_new['Status1']
Y.shape

# Train Test Split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0)

# Training model using Naive bayes classifier

from sklearn.naive_bayes import MultinomialNB
spam_detect_model = MultinomialNB().fit(X_train, y_train)

y_pred=spam_detect_model.predict(X_test)

from sklearn import metrics

print(metrics.classification_report(y_test,y_pred))
print(metrics.confusion_matrix(y_test,y_pred))

y_pred

pickle.dump(spam_detect_model,open('interested.pkl','wb'))






